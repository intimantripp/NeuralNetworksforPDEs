In this section, we outline the basic setup, components, and architecture of feedforward neural 
networks, and describe the standard approach used for training them. While multiple types of neural 
networks exist (e.g., convolutional neural networks, recurrent neural networks), we restrict our 
attention to feedforward neural networks. All references to neural networks in this report refer 
exclusively to this type.

Throughout, we adopt the following notation: vectors are denoted using lowercase bold symbols 
(e.g., $\mathbf{x}$), while matrices are written in uppercase bold (e.g., $\mathbf{W}$). We denote 
the output of the neural network as $\hat{y}$, which approximates a target function $y$.

\subsection{Neural Networks Overview and Architecture}

A feedforward neural network defines a function \( f_\theta : \mathbb{R}^n \to \mathbb{R}^m \), 
parameterised by a collection of weight matrices and bias vectors \( \theta = \{ \mathbf{W}^{(l)},
\mathbf{b}^{(l)} \}_{l=1}^L \). It is trained to approximate a target function \( y : \mathbb{R}^n \to 
\mathbb{R}^m \) using observed or synthetically generated data.

The model takes an input vector \( \mathbf{x} \in \mathbb{R}^n \) and propagates it forward through a
sequence of $L$ layers, each composed of individual units called \emph{neurons}. 

Each \emph{neuron} in a layer performs a simple two-step operation. First, it computes a weighted
sum of its inputs and adds a bias term. Then, it applies a non-linear activation function to the result.
Specifically, if a neuron with weights \( \mathbf{w}^{(l)}_i \in \mathbb{R}^{n_{l-1}} \) and bias 
\( b^{(l)}_i \in \mathbb{R} \) receives an input vector \( \mathbf{z}^{(l-1)} \in \mathbb{R}^{n_{l-1}} \), 
then its output is given by
\[
    z^{(l)}_i = \sigma\left( (\mathbf{w}^{(l)}_i)^\top \mathbf{z}^{(l-1)} + b^{(l)}_i \right),
\]
where \( \sigma: \mathbb{R} \to \mathbb{R} \) is the neuron's fixed activation function.

A layer consists of multiple such neurons, all operating in parallel on the same input vector 
\( \mathbf{z}^{(l-1)} \), each with its own weight vector and bias. The outputs from all neurons in the
layer are collected into a vector \( \mathbf{z}^{(l)} \in \mathbb{R}^{n_l} \), where \( n_l \) denotes 
the number of neurons in layer \( l \). Letting \( \mathbf{W}^{(l)} \in 
\mathbb{R}^{n_l \times n_{l-1}} \) be the matrix whose rows are the individual neuron weight vectors 
\( (\mathbf{w}^{(l)}_i)^\top \), and \( \mathbf{b}^{(l)} \in \mathbb{R}^{n_l} \) the vector of biases, 
we can express the full layer computation compactly as
\[
    \mathbf{z}^{(l)} = \sigma\left( \mathbf{W}^{(l)} \mathbf{z}^{(l-1)} + \mathbf{b}^{(l)} \right),
\]
with the activation function \( \sigma \) now applied componentwise.

The network as a whole consists of a composition of such layers. Starting from the input 
vector \( \mathbf{z}^{(0)} = \mathbf{x} \), each successive layer transforms the output of the previous
one. For a network with \( L \) layers, the computation proceeds recursively:
\[
    \mathbf{z}^{(l)} = \sigma^{(l)}\left( \mathbf{W}^{(l)} \mathbf{z}^{(l-1)} + \mathbf{b}^{(l)} 
    \right), \quad \text{for } l = 1, \dots, L-1,
\]
with the final output given by
\[
    \hat{\mathbf{y}} = f_\theta(\mathbf{x}) = \sigma^{(l)}(\mathbf{W}^{(L)} \mathbf{z}^{(L-1)} + 
    \mathbf{b}^{(L)}).
\]

The total set of network parameters \( \theta = \{ \mathbf{W}^{(l)}, \mathbf{b}^{(l)} \}_{l=1}^L \) 
are learned during training. The architecture is defined by the number of layers \( L \), the number 
of neurons \( n_l \) in each layer, and the choice of activation functions \( \sigma^{(l)} \).
The first and last layers are termed the input and output layers respectively, while intermediate 
layers are termed hidden layers. Figure \ref{fig:nn-architecture} gives an an illustrative diagram of 
a neural network architecture, with each neuron denoted by a circle, and connections indicating the
output of one neuron passed as an input to neurons in the next layer.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{graphics/neural_network_image.png}
    \caption{Illustration of a fully connected feedforward neural network with three hidden layers. 
    Each neuron computes an affine transformation of its inputs followed by a non-linear activation.}
    \label{fig:nn-architecture}
\end{figure}


Common activation functions used include:

\begin{itemize}
    \item \textbf{Hyperbolic tangent (tanh):} \quad 
    \( \sigma(x) = \tanh(x) = \dfrac{e^x - e^{-x}}{e^x + e^{-x}} \), 
    often preferred for its smoothness and differentiability.

    \item \textbf{Rectified Linear Unit (ReLU):} \quad 
    \( \sigma(x) = \max(0, x) \), 
    though not ideal when higher-order derivatives are needed.

    \item \textbf{Leaky ReLU:} \quad 
    \( \sigma(x) = 
    \begin{cases}
        x, & \text{if } x \geq 0 \\
        \alpha x, & \text{if } x < 0
    \end{cases} \), for small \( \alpha > 0 \).
    
    \item \textbf{Sinusoidal:} \quad 
    \( \sigma(x) = \sin(x) \), useful for representing oscillatory functions.
\end{itemize}

In this report we will restrict ourselves to considering these activation functions.


\subsection{Training}\label{sec:nn_training}

The parameters of a neural network — namely the weight matrices and bias vectors 
\( \theta = \{ \mathbf{W}^{(l)}, \mathbf{b}^{(l)} \}_{l=1}^L \) — are learned through a process 
called \emph{training}. The goal of training is to find a parameter set \( \theta \) such that the 
network output \( \hat{y} = f_\theta(\mathbf{x}) \) closely approximates the desired output \( y \) 
over a set of inputs \( \mathbf{x} \in \mathbb{R}^n \).

This is accomplished by defining a \emph{loss function} \( \mathcal{L}(\theta) \) that quantifies the 
discrepancy between the network predictions and the target values across a training dataset. 
In this way, training becomes a minimisation problem, where the goal is to find the parameter 
configuration $\theta^*$ that minimises the loss function $\mathcal{L}$.
When the desired outputs are continuous, a common choice is the mean squared error (MSE):
\[
    \mathcal{L}(\theta) = \frac{1}{N} \sum_{i=1}^{N} \left\| f_\theta(\mathbf{x}^{(i)}) - y^{(i)} \right\|^2,
\]
where \( \{ (\mathbf{x}^{(i)}, y^{(i)}) \}_{i=1}^N \) is the training dataset of input-output pairs.

To minimise the loss function, \( \mathcal{L}(\theta) \), a gradient-based optimisation method is 
used. This requires
computing the gradient of the loss with respect to all network parameters. This is made efficient by 
the \emph{backpropagation algorithm}, which systematically applies the chain rule of calculus to 
compute these derivatives by propagating error signals backward through the layers of the network.

Let us denote the output of layer \( l \) as \( \mathbf{z}^{(l)} \in \mathbb{R}^{n_l} \), computed via
\[
    \mathbf{z}^{(l)} = \sigma^{(l)}\left( \mathbf{a}^{(l)} \right), \quad \text{where} \quad \mathbf{a}^{(l)} = \mathbf{W}^{(l)} \mathbf{z}^{(l-1)} + \mathbf{b}^{(l)},
\]
and \( \sigma^{(l)} \) is the activation function applied componentwise.

Define the error signal at layer \( l \) as
\[
    \boldsymbol{\delta}^{(l)} := \frac{\partial \mathcal{L}}{\partial \mathbf{a}^{(l)}},
\]
which captures the sensitivity of the loss to the pre-activation input at that layer. The error at the final layer \( L \) is computed using the derivative of the loss function with respect to the network output:
\[
    \boldsymbol{\delta}^{(L)} = \nabla_{\hat{\mathbf{y}}} \mathcal{L} \odot \sigma'^{(L)}\left( \mathbf{a}^{(L)} \right),
\]
where \( \odot \) denotes elementwise (Hadamard) product and \( \sigma'^{(L)} \) is the derivative of the activation function at the final layer.

For hidden layers \( l = L-1, \dots, 1 \), the errors are computed recursively using
\[
    \boldsymbol{\delta}^{(l)} = \left( (\mathbf{W}^{(l+1)})^\top \boldsymbol{\delta}^{(l+1)} \right) \odot \sigma'^{(l)}\left( \mathbf{a}^{(l)} \right).
\]

Once the error signals are computed for each layer, the gradients of the loss with respect to the weights and biases are given by
\[
    \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} = \boldsymbol{\delta}^{(l)} (\mathbf{z}^{(l-1)})^\top, \quad
    \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}} = \boldsymbol{\delta}^{(l)}.
\]

These gradients are then used in an optimisation routine, such as Adam or Stochastic Gradient Descent,
to update the parameters:
\[
    \mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}}, \quad
    \mathbf{b}^{(l)} \leftarrow \mathbf{b}^{(l)} - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(l)}},
\]
where \( \alpha > 0 \) is the learning rate.

This optimisation process is repeated over the training dataset in multiple passes 
(epochs), until convergence to a local minimum of the loss function.

This iterative update process constitutes the core of neural network training, and naturally 
divides the procedure into a sequence of \emph{forward passes}, where predictions are computed 
by modifying and propagating forward the inputs, 
and \emph{backward passes}, where gradients are propagated backwards and parameters are updated. Each 
training epoch can be visualised as a forward sweep through the network architecture illustrated
by Figure \ref{fig:nn-architecture}, followed by a backward sweep in which gradients are 
computed via backpropagation and used to adjust the network parameters.

\subsection{Neural Networks for Differential Equations}

When applying neural networks to solve differential equations, the nature of the problem differs 
fundamentally from standard supervised learning tasks. In conventional machine learning, a model 
learns from a dataset of labelled input-output pairs 
\( \{ (\mathbf{x}^{(i)}, y^{(i)}) \} \), and is trained to minimise prediction error on unseen
examples drawn from the same underlying distribution. In contrast, solving a differential equation 
involves finding a function that satisfies a differential constraint and associated boundary or 
initial conditions. No explicit data labels are given; instead, a loss function is constructed that 
penalises violations of the governing equation at selected collocation points. This loss typically 
involves computing derivatives of the network output with respect to its input — a task well suited 
to modern automatic differentiation frameworks.

Another key difference is that supervised learning often involves noisy training data due to 
measurement error or system variability. Neural networks in that context are trained to generalise 
despite this uncertainty. Differential equation problems, by contrast, are typically deterministic: 
the equations are known exactly, and the solution is expected to satisfy them precisely. As a result, 
the concepts of overfitting and underfitting take on new meaning — referring to how well the learned 
function satisfies the equation and constraints, rather than its generalisation to unseen data.

Before analysing more complex problem classes, we conclude this section with a simple illustrative 
example. This serves to demonstrate the methodology outlined, reinforce the distinctions 
from standard supervised learning highlighted above, and validate our implementation method. 
It is also representative of the general method applied in subsequent sections when training 
neural networks in this report.

We consider the boundary value problem
\[
\begin{aligned}
    y''(x) &= 2, \quad 0 < x < 1, \\
    y(0) &= 1, \\
    y(1) &= 1,
\end{aligned}
\]
whose exact solution is \( y(x) = 1 + x(1 - x) \).

We approximate this solution using a neural network \( \hat{y}(x) = f_\theta(x) \), where 
\( \theta \) denotes the trainable weights and biases. The loss function penalises both 
deviations from the differential equation and violations of the boundary conditions:
\begin{equation}
\label{eq:prelim_loss_func}
\mathcal{L}(\theta) = \sum_{k=1}^N \left( \hat{y}''(x_k) - 2 \right)^2 
+ \gamma \left( \hat{y}(0) - 1 \right)^2 
+ \gamma \left( \hat{y}(1) - 1 \right)^2,
\end{equation}
where \( \{x_k\}_{k=1}^N \subset (0,1) \) are collocation points, and \( \gamma > 0 \) is a penalty 
coefficient.

Our model architecture is illustrated in Figure~\ref{fig:preamble_neural_net}. It consists of a 
fully connected feedforward neural network with two hidden layers, each containing five neurons. 
The activation function in the hidden layers is the hyperbolic tangent, \( \tanh(x) \), chosen for 
its smoothness and differentiability. No activation is used in the output layer, consistent 
with regression tasks involving continuous outputs.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      % Input layer
      \matrix[layer] (input) {
        |[neuron]| $x$ \\
      };

      % Hidden layer 1
      \matrix[layer, right=of input] (hidden1) {
        |[neuron]| $h^{(1)}_1$ \\
        |[neuron]| $h^{(1)}_2$ \\
        |[neuron]| $h^{(1)}_3$ \\
        |[neuron]| $h^{(1)}_4$ \\
        |[neuron]| $h^{(1)}_5$ \\
      };

      % Hidden layer 2
      \matrix[layer, right=of hidden1] (hidden2) {
        |[neuron]| $h^{(2)}_1$ \\
        |[neuron]| $h^{(2)}_2$ \\
        |[neuron]| $h^{(2)}_3$ \\
        |[neuron]| $h^{(2)}_4$ \\
        |[neuron]| $h^{(2)}_5$ \\
      };

      % Output layer
      \matrix[layer, right=of hidden2] (output) {
        |[neuron]| $\hat{y}$ \\
      };

      % Connections: input → hidden1
      \foreach \j in {1,2,3,4,5}
        \draw[connect] (input-1-1) -- (hidden1-\j-1);

      % Connections: hidden1 → hidden2
      \foreach \i in {1,2,3,4,5}
        \foreach \j in {1,2,3,4,5}
          \draw[connect] (hidden1-\i-1) -- (hidden2-\j-1);

      % Connections: hidden2 → output
      \foreach \i in {1,2,3,4,5}
        \draw[connect] (hidden2-\i-1) -- (output-1-1);

    \end{tikzpicture}
    \caption{Fully connected feedforward neural network used to approximate the solution \( \hat{y}(x) \). The network takes a scalar input \( x \), passes it through two hidden layers with five neurons each, and outputs a scalar prediction.}
    \label{fig:preamble_neural_net}
\end{figure}

The network is trained on 20 equally spaced points in \([0, 1]\), including the boundary values 
at \( x = 0 \) and \( x = 1 \). We set \( \gamma = 1 \) in the loss function~\eqref{eq:prelim_loss_func}, 
and optimise using the Adam algorithm with a fixed learning rate of \( \alpha = 0.001 \). Training 
proceeds for 2000 epochs.

Figure~\ref{fig:prelim_nn_diagnostics} shows the training diagnostics. The left panel illustrates the 
convergence of the loss during training. The right panel compares the network's prediction to the 
true solution. The neural network recovers the solution accurately across the domain, with minor 
deviations near the centre. By increasing the number of epochs we would likely have further have further 
reduced these deviations.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/prelim_nn_loss_curve.png}
        \caption{Training loss over 2000 epochs.}
        \label{fig:prelim_nn_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/prelim_nn_example.png}
        \caption{Neural network prediction vs true solution.}
        \label{fig:prelim_nn_model}
    \end{subfigure}
    \caption{Training diagnostics for the neural network solution to the ODE.}
    \label{fig:prelim_nn_diagnostics}
\end{figure}
