\relax 
\abx@aux@refcontext{none/global//global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec: intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{1}{section.2}\protected@file@percent }
\newlabel{sec:preliminaries}{{2}{1}{Preliminaries}{section.2}{}}
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural Networks Overview and Architecture}{2}{subsection.2.1}\protected@file@percent }
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\abx@aux@cite{0}{ramachandran2017searching}
\abx@aux@segm{0}{0}{ramachandran2017searching}
\abx@aux@cite{0}{ramachandran2017searching}
\abx@aux@segm{0}{0}{ramachandran2017searching}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of a fully connected feedforward neural network with three hidden layers. Each neuron computes an affine transformation of its inputs followed by a non-linear activation.}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nn-architecture}{{1}{3}{Illustration of a fully connected feedforward neural network with three hidden layers. Each neuron computes an affine transformation of its inputs followed by a non-linear activation}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training}{4}{subsection.2.2}\protected@file@percent }
\newlabel{sec:nn_training}{{2.2}{4}{Training}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural Networks for Differential Equations}{5}{subsection.2.3}\protected@file@percent }
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\newlabel{eq:intro_problem}{{1}{6}{Neural Networks for Differential Equations}{equation.2.1}{}}
\newlabel{eq:prelim_loss_func}{{2}{6}{Neural Networks for Differential Equations}{equation.2.2}{}}
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Fully connected feedforward neural network used to approximate the solution to \eqref  {eq:intro_problem}. The network takes a scalar input \( x \), passes it through two hidden layers with five neurons each, and outputs a scalar prediction.}}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:preamble_neural_net}{{2}{7}{Fully connected feedforward neural network used to approximate the solution to \eqref {eq:intro_problem}. The network takes a scalar input \( x \), passes it through two hidden layers with five neurons each, and outputs a scalar prediction}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Ordinary Differential Equations}{7}{section.3}\protected@file@percent }
\newlabel{sec:odes}{{3}{7}{Ordinary Differential Equations}{section.3}{}}
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\abx@aux@cite{0}{paszke2017automatic}
\abx@aux@segm{0}{0}{paszke2017automatic}
\newlabel{fig:prelim_nn_loss}{{3a}{8}{Training loss over 2000 epochs}{figure.caption.3}{}}
\newlabel{sub@fig:prelim_nn_loss}{{a}{8}{Training loss over 2000 epochs}{figure.caption.3}{}}
\newlabel{fig:prelim_nn_model}{{3b}{8}{Neural network prediction vs true solution}{figure.caption.3}{}}
\newlabel{sub@fig:prelim_nn_model}{{b}{8}{Neural network prediction vs true solution}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training diagnostics for the neural network solution to the ODE.}}{8}{figure.caption.3}\protected@file@percent }
\newlabel{fig:prelim_nn_diagnostics}{{3}{8}{Training diagnostics for the neural network solution to the ODE}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Initial Value Problems}{9}{subsection.3.1}\protected@file@percent }
\newlabel{sec:IVPs}{{3.1}{9}{Initial Value Problems}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Exponential Decay}{9}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Periodic Solution}{9}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{10}{figure.caption.4}\protected@file@percent }
\newlabel{fig:expdecay_sidebyside}{{4}{10}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of architectural performance for the oscillatory problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit for each activation function.}}{11}{figure.caption.5}\protected@file@percent }
\newlabel{fig:ivp_periodic_sidebyside}{{5}{11}{Comparison of architectural performance for the oscillatory problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit for each activation function}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Singular Solution}{12}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Boundary Value Problems}{12}{subsection.3.2}\protected@file@percent }
\newlabel{sec:BVPs}{{3.2}{12}{Boundary Value Problems}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of architectural performance for the singular solution problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{13}{figure.caption.6}\protected@file@percent }
\newlabel{fig:ivp_singular_sidebyside}{{6}{13}{Comparison of architectural performance for the singular solution problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Poisson Problem}{14}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Piecewise Forcing}{14}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of architectural performance for the Poisson BVP using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{15}{figure.caption.7}\protected@file@percent }
\newlabel{fig:bvp_poisson_sidebyside}{{7}{15}{Comparison of architectural performance for the Poisson BVP using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of architectural performance for the piecewise-forced BVP using two activation functions. Each column shows the MSE heatmap (log scale) and best-fit solution.}}{16}{figure.caption.8}\protected@file@percent }
\newlabel{fig:bvp_piecewise_sidebyside}{{8}{16}{Comparison of architectural performance for the piecewise-forced BVP using two activation functions. Each column shows the MSE heatmap (log scale) and best-fit solution}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Extension: Partial Differential Equations}{16}{section.4}\protected@file@percent }
\newlabel{sec:pdes}{{4}{16}{Extension: Partial Differential Equations}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Architectural Performance on the Poisson Problem}{17}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Robustness to Reduced Training Data}{17}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of neural network performance on the two-dimensional Poisson problem using the $\tanh $ and Swish activation functions. Top row: log-scaled MSE heatmaps across architectural configurations. Middle row: predicted solutions from the best-performing networks. Bottom row: corresponding absolute error plots.}}{18}{figure.caption.9}\protected@file@percent }
\newlabel{fig:pde_comparison}{{9}{18}{Comparison of neural network performance on the two-dimensional Poisson problem using the $\tanh $ and Swish activation functions. Top row: log-scaled MSE heatmaps across architectural configurations. Middle row: predicted solutions from the best-performing networks. Bottom row: corresponding absolute error plots}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Mean squared error heatmap showing how approximation accuracy varies with the number of internal and boundary training points.}}{19}{figure.caption.10}\protected@file@percent }
\newlabel{fig:training_point_comparison}{{10}{19}{Mean squared error heatmap showing how approximation accuracy varies with the number of internal and boundary training points}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{19}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{19}{Conclusion}{section.5}{}}
\newlabel{fig:prediction_80}{{\caption@xref {fig:prediction_80}{ on input line 81}}{20}{Robustness to Reduced Training Data}{figure.caption.11}{}}
\newlabel{sub@fig:prediction_80}{{}{20}{Robustness to Reduced Training Data}{figure.caption.11}{}}
\newlabel{fig:prediction_100}{{\caption@xref {fig:prediction_100}{ on input line 89}}{20}{Robustness to Reduced Training Data}{figure.caption.11}{}}
\newlabel{sub@fig:prediction_100}{{}{20}{Robustness to Reduced Training Data}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Comparison of neural network predictions illustrating improved accuracy with moderate increases in training data.}}{20}{figure.caption.11}\protected@file@percent }
\newlabel{fig:stacked_predictions}{{11}{20}{Comparison of neural network predictions illustrating improved accuracy with moderate increases in training data}{figure.caption.11}{}}
\abx@aux@read@bbl@mdfivesum{8A01DFA1435E4A88A5AD545A6F0AC47A}
\abx@aux@defaultrefcontext{0}{goodfellow2016deep}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ramachandran2017searching}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{paszke2017automatic}{none/global//global/global}
\gdef \@abspage@last{22}
