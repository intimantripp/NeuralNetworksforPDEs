\relax 
\abx@aux@refcontext{none/global//global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec: intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}\protected@file@percent }
\newlabel{sec:preliminaries}{{2}{2}{Preliminaries}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural Networks Overview and Architecture}{2}{subsection.2.1}\protected@file@percent }
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of a fully connected feedforward neural network with three hidden layers. Each neuron computes an affine transformation of its inputs followed by a non-linear activation.}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nn-architecture}{{1}{3}{Illustration of a fully connected feedforward neural network with three hidden layers. Each neuron computes an affine transformation of its inputs followed by a non-linear activation}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training}{4}{subsection.2.2}\protected@file@percent }
\newlabel{sec:nn_training}{{2.2}{4}{Training}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural Networks for Differential Equations}{5}{subsection.2.3}\protected@file@percent }
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\newlabel{eq:prelim_loss_func}{{1}{6}{Neural Networks for Differential Equations}{equation.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Fully connected feedforward neural network used to approximate the solution \( \hat  {y}(x) \). The network takes a scalar input \( x \), passes it through two hidden layers with five neurons each, and outputs a scalar prediction.}}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:preamble_neural_net}{{2}{7}{Fully connected feedforward neural network used to approximate the solution \( \hat {y}(x) \). The network takes a scalar input \( x \), passes it through two hidden layers with five neurons each, and outputs a scalar prediction}{figure.caption.2}{}}
\newlabel{fig:prelim_nn_loss}{{3a}{7}{Training loss over 2000 epochs}{figure.caption.3}{}}
\newlabel{sub@fig:prelim_nn_loss}{{a}{7}{Training loss over 2000 epochs}{figure.caption.3}{}}
\newlabel{fig:prelim_nn_model}{{3b}{7}{Neural network prediction vs true solution}{figure.caption.3}{}}
\newlabel{sub@fig:prelim_nn_model}{{b}{7}{Neural network prediction vs true solution}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training diagnostics for the neural network solution to the ODE.}}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:prelim_nn_diagnostics}{{3}{7}{Training diagnostics for the neural network solution to the ODE}{figure.caption.3}{}}
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\abx@aux@cite{0}{paszke2017automatic}
\abx@aux@segm{0}{0}{paszke2017automatic}
\@writefile{toc}{\contentsline {section}{\numberline {3}Ordinary Differential Equations}{8}{section.3}\protected@file@percent }
\newlabel{sec:odes}{{3}{8}{Ordinary Differential Equations}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Initial Value Problems}{9}{subsection.3.1}\protected@file@percent }
\newlabel{sec:IVPs}{{3.1}{9}{Initial Value Problems}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Exponential Decay}{9}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Periodic Solution}{10}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Singular Solution}{10}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Boundary Value Problems}{10}{subsection.3.2}\protected@file@percent }
\newlabel{sec:BVPs}{{3.2}{10}{Boundary Value Problems}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Poisson Problem}{11}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Piecewise Forcing}{12}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Extension: Partial Differential Equations}{12}{section.4}\protected@file@percent }
\newlabel{sec:pdes}{{4}{12}{Extension: Partial Differential Equations}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{12}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{12}{Conclusion}{section.5}{}}
\newlabel{fig:expdecay_tanh_column}{{4a}{13}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.5}{}}
\newlabel{sub@fig:expdecay_tanh_column}{{a}{13}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.5}{}}
\newlabel{fig:expdecay_relu_column}{{4b}{13}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.5}{}}
\newlabel{sub@fig:expdecay_relu_column}{{b}{13}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{13}{figure.caption.5}\protected@file@percent }
\newlabel{fig:expdecay_sidebyside}{{4}{13}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.5}{}}
\newlabel{fig:ivp_periodic_tanh}{{5a}{14}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.7}{}}
\newlabel{sub@fig:ivp_periodic_tanh}{{a}{14}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.7}{}}
\newlabel{fig:ivp_periodic_relu}{{5b}{14}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.7}{}}
\newlabel{sub@fig:ivp_periodic_relu}{{b}{14}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{14}{figure.caption.7}\protected@file@percent }
\newlabel{fig:ivp_periodic_sidebyside}{{5}{14}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.7}{}}
\newlabel{fig:ivp_periodic_tanh}{{6a}{15}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.9}{}}
\newlabel{sub@fig:ivp_periodic_tanh}{{a}{15}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.9}{}}
\newlabel{fig:ivp_periodic_relu}{{6b}{15}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.9}{}}
\newlabel{sub@fig:ivp_periodic_relu}{{b}{15}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{15}{figure.caption.9}\protected@file@percent }
\newlabel{fig:ivp_periodic_sidebyside}{{6}{15}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.9}{}}
\newlabel{fig:ivp_periodic_tanh}{{7a}{16}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.11}{}}
\newlabel{sub@fig:ivp_periodic_tanh}{{a}{16}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.11}{}}
\newlabel{fig:ivp_periodic_relu}{{7b}{16}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.11}{}}
\newlabel{sub@fig:ivp_periodic_relu}{{b}{16}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:bvp_poisson_sidebyside}{{7}{16}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.11}{}}
\newlabel{fig:ivp_periodic_tanh}{{8a}{17}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.13}{}}
\newlabel{sub@fig:ivp_periodic_tanh}{{a}{17}{\textbf {Tanh activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.13}{}}
\newlabel{fig:ivp_periodic_relu}{{8b}{17}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.13}{}}
\newlabel{sub@fig:ivp_periodic_relu}{{b}{17}{\textbf {ReLU activation.} Heatmap (top), best fit (middle), and worst fit (bottom)}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{17}{figure.caption.13}\protected@file@percent }
\newlabel{fig:bvp_piecewise_sidebyside}{{8}{17}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.13}{}}
\abx@aux@read@bbl@mdfivesum{18AC1F74DFCFB6AC7DE50FF70581EBC4}
\abx@aux@defaultrefcontext{0}{goodfellow2016deep}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{paszke2017automatic}{none/global//global/global}
\gdef \@abspage@last{18}
