\relax 
\abx@aux@refcontext{none/global//global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec: intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}\protected@file@percent }
\newlabel{sec:preliminaries}{{2}{2}{Preliminaries}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Neural Networks Overview and Architecture}{2}{subsection.2.1}\protected@file@percent }
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of a fully connected feedforward neural network with three hidden layers. Each neuron computes an affine transformation of its inputs followed by a non-linear activation.}}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:nn-architecture}{{1}{3}{Illustration of a fully connected feedforward neural network with three hidden layers. Each neuron computes an affine transformation of its inputs followed by a non-linear activation}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training}{4}{subsection.2.2}\protected@file@percent }
\newlabel{sec:nn_training}{{2.2}{4}{Training}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural Networks for Differential Equations}{5}{subsection.2.3}\protected@file@percent }
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\newlabel{eq:prelim_loss_func}{{1}{6}{Neural Networks for Differential Equations}{equation.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Fully connected feedforward neural network used to approximate the solution \( \hat  {y}(x) \). The network takes a scalar input \( x \), passes it through two hidden layers with five neurons each, and outputs a scalar prediction.}}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:preamble_neural_net}{{2}{7}{Fully connected feedforward neural network used to approximate the solution \( \hat {y}(x) \). The network takes a scalar input \( x \), passes it through two hidden layers with five neurons each, and outputs a scalar prediction}{figure.caption.2}{}}
\newlabel{fig:prelim_nn_loss}{{3a}{7}{Training loss over 2000 epochs}{figure.caption.3}{}}
\newlabel{sub@fig:prelim_nn_loss}{{a}{7}{Training loss over 2000 epochs}{figure.caption.3}{}}
\newlabel{fig:prelim_nn_model}{{3b}{7}{Neural network prediction vs true solution}{figure.caption.3}{}}
\newlabel{sub@fig:prelim_nn_model}{{b}{7}{Neural network prediction vs true solution}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training diagnostics for the neural network solution to the ODE.}}{7}{figure.caption.3}\protected@file@percent }
\newlabel{fig:prelim_nn_diagnostics}{{3}{7}{Training diagnostics for the neural network solution to the ODE}{figure.caption.3}{}}
\abx@aux@cite{0}{goodfellow2016deep}
\abx@aux@segm{0}{0}{goodfellow2016deep}
\abx@aux@cite{0}{paszke2017automatic}
\abx@aux@segm{0}{0}{paszke2017automatic}
\@writefile{toc}{\contentsline {section}{\numberline {3}Ordinary Differential Equations}{8}{section.3}\protected@file@percent }
\newlabel{sec:odes}{{3}{8}{Ordinary Differential Equations}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Initial Value Problems}{9}{subsection.3.1}\protected@file@percent }
\newlabel{sec:IVPs}{{3.1}{9}{Initial Value Problems}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Exponential Decay}{9}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{10}{figure.caption.4}\protected@file@percent }
\newlabel{fig:expdecay_sidebyside}{{4}{10}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Periodic Solution}{11}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Singular Solution}{11}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig:ivp_periodic_sidebyside}{{5}{12}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{13}{figure.caption.6}\protected@file@percent }
\newlabel{fig:ivp_singular_sidebyside}{{6}{13}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Boundary Value Problems}{14}{subsection.3.2}\protected@file@percent }
\newlabel{sec:BVPs}{{3.2}{14}{Boundary Value Problems}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Poisson Problem}{14}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{15}{figure.caption.7}\protected@file@percent }
\newlabel{fig:bvp_poisson_sidebyside}{{7}{15}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Piecewise Forcing}{16}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Extension: Partial Differential Equations}{16}{section.4}\protected@file@percent }
\newlabel{sec:pdes}{{4}{16}{Extension: Partial Differential Equations}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit.}}{17}{figure.caption.8}\protected@file@percent }
\newlabel{fig:bvp_piecewise_sidebyside}{{8}{17}{Comparison of architectural performance for the exponential decay problem using two activation functions. Each column shows the MSE heatmap with a log error scale, the best network fit, and the worst network fit}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Architectural Performance on the Poisson Problem}{18}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{18}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{18}{Conclusion}{section.5}{}}
\abx@aux@read@bbl@mdfivesum{18AC1F74DFCFB6AC7DE50FF70581EBC4}
\abx@aux@defaultrefcontext{0}{goodfellow2016deep}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{paszke2017automatic}{none/global//global/global}
\gdef \@abspage@last{19}
